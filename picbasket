#!/usr/bin/env python3

import os
import sys
import time
import shutil
import json
import pickle
from collections import defaultdict
from multiprocessing import Pool, TimeoutError, cpu_count
from PIL import Image
import imagehash
import exifread

def error(*msg):
    print('Error:', *msg, file=sys.stderr)
    exit(1)

def warn(*msg):
    print('Warning:', *msg, file=sys.stderr)

def info(*msg):
    print('Info:', *msg)

def load_config(cfgfile, args):
    # Defaults
    config = {
        'file_naming': '%Y/%B/%d_{filename}_{resy}', # time.strftime format specifiers (%) plus the following keys ({}): filename, resx, resy
        'duplicate_handling': 'highest_resolution', # lowest_resolution, newest, oldest, none
        'delete_input': False,
        'persist_input': False,
        'inputs': [],
        'output': '',
        'threads': cpu_count()
    }
    # Override defaults with config file settings
    if cfgfile and os.path.exists(cfgfile):
        info('Loading configuration from', cfgfile)
        with open(cfgfile, 'r') as fd:
            config.update(json.load(fd))
    # Override or append config file settings with CLI arguments
    if args:
        config['inputs'].extend([os.path.abspath(d) for d in args.dirs])
        if args.output:
            config['output'] = os.path.abspath(args.output)
        config['threads'] = args.threads
        config['persist_input'] = args.persist_input
        config['delete_input'] = args.delete_input
    return config

def get_config(outdir):
    f = os.path.join(os.path.abspath(outdir), '.picbasket.cfg')
    if os.path.exists(f):
        return f
    return ''

def save_config(config):
    f = os.path.join(os.path.abspath(config['output']), '.picbasket.cfg')
    # Don't store certain non-persistent settings, e.g. threads as it's machine dependent
    persistent_config = config.copy()
    persistent_config.pop('threads', None)
    persistent_config.pop('persist_input', None)
    if not config['persist_input']:
        persistent_config.pop('inputs', None)
    with open(f, 'w', encoding='utf-8') as fd:
        json.dump(persistent_config, fd, ensure_ascii=False, indent=4)

def load_db(config):
    f = os.path.join(os.path.abspath(config['output']), '.picbasket.db')
    if os.path.exists(f):
        with open(f, 'rb') as fd:
            return pickle.load(fd)
    else:
        return defaultdict(list)

def save_db(config, db):
    f = os.path.join(os.path.abspath(config['output']), '.picbasket.db')
    with open(f, 'wb') as fd:
        pickle.dump(db, fd)

def get_timestamp(fd, f):
    # Use EXIF data if available, else fallback to file modification time
    tags = exifread.process_file(fd, details=False, stop_tag='Image DateTime')
    if 'Image DateTime' in tags:
        return int(time.mktime(time.strptime(str(tags['Image DateTime']), '%Y:%m:%d %H:%M:%S')))
    else:
        return int(os.path.getmtime(f))

def hash_img(f):
    with open(f, 'rb') as fd:
        try:
            img = Image.open(fd)
        except:
            # This is not an image, skip this file
            return ['', None, '']
        res = img.size
        h = str(imagehash.phash(img))
        ts = get_timestamp(fd, f)
        return [h, res, ts]

def discover(config, db):
    with Pool(processes=config['threads']) as pool:
        for d in config['inputs']:
            p = os.path.abspath(d)
            if not os.path.isdir(p):
                continue
            for root, dirs, files in os.walk(p):
                for base in files:
                    f = os.path.join(root, base)
                    try:
                        h, res, ts = pool.apply_async(hash_img, (f,)).get(timeout=1000)
                    except TimeoutError:
                        warn('Timeout hashing file', f)
                        continue
                    if h:
                        db[h].append([f, res, ts])

def name(config, img):
    template = time.strftime(config['file_naming'], time.localtime(img[2]))
    bn, ext = os.path.splitext(os.path.basename(img[0]))
    # TODO: sanitize bn (spaces, special chars, etc.)
    return os.path.join(config['output'], template.format(filename=bn, resx=img[1][0], resy=img[1][1]) + ext)

def resolve(config, imgs, h, newdb):
    strategy = config['duplicate_handling']
    if strategy == 'none':
        newdb[h] = imgs
        return [[img[0], name(config, img)] for img in imgs]
    candidate = []
    for img in imgs:
        if not candidate:
            candidate = img
        elif strategy == 'highest_resolution':
            if img[1][0] > candidate[1][0]: # TODO: check both dimensions, or assume scaled same?
                candidate = img
        elif strategy == 'lowest_resolution':
            if img[1][0] < candidate[1][0]: # TODO: check both dimensions, or assume scaled same?
                candidate = img
        elif strategy == 'newest':
            if img[2] > candidate[2]:
                candidate = img
        elif strategy == 'oldest':
            if img[2] < candidate[2]:
                candidate = img
    src = candidate[0]
    if src.startswith(config['output']): # no action needed, the one in the output is already the selected one
        newdb[h].append([src, candidate[1], candidate[2]])
        return []
    dst = name(config, candidate)
    newdb[h].append([dst, candidate[1], candidate[2]])
    return [[src, dst]]

def copy(src, dst, removesrc):
    try:
        os.makedirs(os.path.dirname(dst), exist_ok=True)
        if removesrc:
            shutil.move(src, dst)
        else:
            shutil.copy2(src, dst)
    except:
        warn('Failed to copy', src, 'to', dst)

def migrate(config, db):
    os.makedirs(os.path.abspath(config['output']), mode=0o755, exist_ok=True)
    newdb = defaultdict(list)
    with Pool(processes=min(config['threads'], 4)) as pool: # cap at 4 threads for IO bandwidth
        for h, imgs in db.items():
            for src, dst in resolve(config, imgs, h, newdb):
                try:
                   pool.apply_async(copy, (src, dst, config['delete_input'],)).get(timeout=10000)
                except TimeoutError:
                   warn('Timeout copying file', src, 'to', dst)
    return newdb

def main_cli():
    # Process arguments, set up config dict
    import argparse
    parser = argparse.ArgumentParser(description='TODO')
    parser.add_argument('dirs', nargs='*', help='Directories to search for images')
    parser.add_argument('-c', '--config', type=str, help='Configuration file')
    parser.add_argument('-o', '--output', type=str, help='Output directory')
    parser.add_argument('-j', '--threads', type=int, default=cpu_count(), help='Number of threads to use')
    parser.add_argument('--persist-input', action='store_true', help='Save input directories to configuration so that they are always indexed')
    parser.add_argument('--delete-input', action='store_true', help='Delete input files after copying to output directory')
    args = parser.parse_args()
    if args.config:
        cfgfile = os.path.abspath(args.config)
    elif args.output:
        cfgfile = get_config(args.output)
    else:
        cfgfile = ''
    config = load_config(cfgfile, args)
    if len(config['inputs']) == 0:
        error('No input directory given')
    if config['output'] == '':
        error('No output directory given')
    # Load, discover, migrate and save
    db = load_db(config)
    discover(config, db)
    import pprint as pp # temporary
    pp.pprint(db) # temporary
    db = migrate(config, db)
    pp.pprint(db) # temporary
    save_db(config, db)
    save_config(config)

def main_gui():
    # TODO: GUI
    return

def main():
    # TODO: GUI
    main_cli()

if __name__ == '__main__':
    main()
